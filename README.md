# QConcursos Scraper com Interface Web ğŸ•·ï¸ğŸ“Š

Scraper automatizado para extrair questÃµes do QConcursos e gerar mapas mentais no formato Freeplane (.mm), agora com **interface web em tempo real** para monitoramento completo do processo!

## âœ¨ Funcionalidades

### ğŸ”„ Scraping Automatizado
- **Login automÃ¡tico** com mÃºltiplos seletores de fallback
- **Processamento de mÃºltiplas URLs** em lote
- **ExtraÃ§Ã£o inteligente** de questÃµes, gabaritos e comentÃ¡rios
- **GeraÃ§Ã£o automÃ¡tica** de mapas mentais Freeplane

### ğŸ“Š Interface Web em Tempo Real
- **Monitor visual** com dashboard moderno e responsivo
- **Logs em tempo real** via WebSocket
- **Progresso visual** com barras de progresso e estatÃ­sticas
- **Screenshots automÃ¡ticos** quando ocorrem erros
- **Status detalhado** de cada URL processada

### ğŸ³ Deploy com Docker
- **ContainerizaÃ§Ã£o completa** com Docker Compose
- **Redis integrado** para comunicaÃ§Ã£o em tempo real
- **Volumes persistentes** para logs, screenshots e outputs

## ğŸš€ Como Usar

### ğŸ“‹ PrÃ©-requisitos

1. **Python 3.8+** ou **Docker**
2. **Credenciais do QConcursos**
3. **Arquivo urls.txt** com as URLs a processar

### ğŸ”§ ConfiguraÃ§Ã£o RÃ¡pida

#### 1. **Configure as credenciais:**
```bash
python setup_env.py
```

#### 2. **Crie o arquivo urls.txt:**
```
https://www.qconcursos.com/questoes-de-concursos/prova/123
https://www.qconcursos.com/questoes-de-concursos/prova/456
```

### ğŸ–¥ï¸ Executar Localmente

#### **OpÃ§Ã£o 1: Interface Completa (Recomendado)**
```bash
# Instalar dependÃªncias
pip install -r requirements.txt

# Executar interface web + scraper
python start_monitor.py

# Acessar: http://localhost:5000
```

#### **OpÃ§Ã£o 2: Apenas Interface Web**
```bash
python start_monitor.py --mode web
```

#### **OpÃ§Ã£o 3: Apenas Scraper**
```bash
python start_monitor.py --mode scraper
```

#### **OpÃ§Ã£o 4: Teste de Login**
```bash
python test_login.py
```

### ğŸ³ Executar com Docker

#### **MÃ©todo 1: Interface Completa**
```bash
# Subir todos os serviÃ§os
docker-compose up

# Acessar: http://localhost:5000
```

#### **MÃ©todo 2: Apenas Interface Web**
```bash
docker-compose up redis web-interface
```

## ğŸ“Š Interface Web - Funcionalidades

### ğŸ¯ Dashboard Principal
- **Status do Sistema**: Running/Stopped com Ã­cones visuais
- **Tempo Decorrido**: CronÃ´metro em tempo real
- **Progresso Geral**: Barra de progresso com porcentagem
- **EstatÃ­sticas**: URLs processadas, nÃ³s extraÃ­dos, etc.

### ğŸ“ Logs em Tempo Real
- **Auto-scroll** configurÃ¡vel
- **Filtros por nÃ­vel**: INFO, SUCCESS, WARNING, ERROR
- **Timestamps** precisos
- **ColorizaÃ§Ã£o** por tipo de log

### ğŸ“¸ Galeria de Screenshots
- **Screenshots automÃ¡ticos** em caso de erro
- **VisualizaÃ§Ã£o em grid**
- **Clique para ampliar**
- **OrganizaÃ§Ã£o cronolÃ³gica**

### ğŸ“ˆ Monitoramento AvanÃ§ado
- **URL atual** sendo processada
- **Progress tracking** detalhado
- **WebSocket** para atualizaÃ§Ãµes instantÃ¢neas
- **Responsivo** para mobile e desktop

## ğŸ”§ ConfiguraÃ§Ã£o AvanÃ§ada

### âš™ï¸ config.py
```python
# ConfiguraÃ§Ãµes de scraping
SCRAPING_CONFIG = {
    "headless": False,          # True para execuÃ§Ã£o em background
    "timeout": 30000,           # Timeout de login (ms)
    "scroll_step": 1500,        # Pixels por scroll
    "url_pause": 1,             # Pausa entre URLs (s)
}

# ConfiguraÃ§Ãµes de debug
DEBUG_CONFIG = {
    "screenshot_on_error": True,  # Screenshots automÃ¡ticos
    "verbose_logging": True,      # Logs detalhados
    "save_raw_html": True,       # Salvar HTML para debug
}
```

### ğŸŒ Interface Web
- **Porta**: 5000 (configurÃ¡vel)
- **Redis**: ComunicaÃ§Ã£o em tempo real
- **WebSocket**: Logs instantÃ¢neos
- **API REST**: Status e dados

## ğŸ“ Estrutura de Arquivos

```
seleniu/
â”œâ”€â”€ ğŸ“Š Interface Web
â”‚   â”œâ”€â”€ web_interface.py          # Servidor Flask principal
â”‚   â”œâ”€â”€ templates/monitor.html    # Interface HTML
â”‚   â””â”€â”€ start_monitor.py          # Script de inicializaÃ§Ã£o
â”œâ”€â”€ ğŸ•·ï¸ Scraping
â”‚   â”œâ”€â”€ scraper.py                # Scraper principal
â”‚   â”œâ”€â”€ config.py                 # ConfiguraÃ§Ãµes
â”‚   â”œâ”€â”€ freeplane.py              # Gerador XML
â”‚   â””â”€â”€ test_login.py             # Teste de login
â”œâ”€â”€ ğŸ”§ ConfiguraÃ§Ã£o
â”‚   â”œâ”€â”€ setup_env.py              # Configurador de credenciais
â”‚   â”œâ”€â”€ requirements.txt          # DependÃªncias Python
â”‚   â”œâ”€â”€ Dockerfile                # Container
â”‚   â””â”€â”€ docker-compose.yml        # OrquestraÃ§Ã£o
â”œâ”€â”€ ğŸ“‚ Dados
â”‚   â”œâ”€â”€ urls.txt                  # URLs a processar
â”‚   â”œâ”€â”€ .env                      # Credenciais (criar)
â”‚   â”œâ”€â”€ logs/                     # Logs automÃ¡ticos
â”‚   â”œâ”€â”€ screenshots/              # Screenshots de erro
â”‚   â””â”€â”€ output/                   # XMLs gerados
```

## ğŸ” Monitoramento e Debug

### ğŸ“Š Acesso Ã  Interface
- **URL**: http://localhost:5000
- **Auto-refresh**: Dados atualizados a cada 5s
- **WebSocket**: Logs instantÃ¢neos

### ğŸ› Debugging
- **Screenshots**: Salvos automaticamente em erros
- **Logs detalhados**: Todos os passos do processo
- **HTML bruto**: Para anÃ¡lise de seletores
- **Modo visual**: Navegador visÃ­vel para debug

### ğŸ“ Logs Estruturados
```
[12:34:56] INFO: Iniciando scraping do QConcursos...
[12:34:58] SUCCESS: Login realizado com sucesso!
[12:35:02] INFO: Scraping URL 1/5: https://...
[12:35:10] SUCCESS: ExtraÃ­dos 25 nÃ³s desta URL
```

## ğŸš¨ SoluÃ§Ã£o de Problemas

### âŒ Erro de Login
1. **Verificar credenciais**: `python setup_env.py`
2. **Testar login**: `python test_login.py`
3. **Verificar seletores**: Interface web mostra elementos encontrados

### âŒ Timeout na Interface
1. **Verificar Redis**: `docker-compose logs redis`
2. **Verificar logs**: Pasta `logs/`
3. **Reiniciar serviÃ§os**: `docker-compose restart`

### âŒ Seletores Desatualizados
1. **Debug visual**: `headless: False` no config.py
2. **Analisar HTML**: Screenshots e HTML salvos automaticamente
3. **Atualizar seletores**: config.py > SEL

## ğŸ“¦ DependÃªncias

### ğŸ Python
- **playwright**: AutomaÃ§Ã£o de navegador
- **flask**: Servidor web
- **flask-socketio**: WebSocket para tempo real
- **redis**: Cache e comunicaÃ§Ã£o
- **python-dotenv**: VariÃ¡veis de ambiente

### ğŸ³ Docker
- **Python 3.11**: Base
- **Redis 7**: Cache e pub/sub
- **Volumes**: PersistÃªncia de dados

## ğŸ¯ PrÃ³ximas Funcionalidades

- [ ] **API REST** completa para integraÃ§Ã£o
- [ ] **Agendamento** de execuÃ§Ãµes
- [ ] **MÃºltiplos sites** de concurso
- [ ] **Export** em mÃºltiplos formatos
- [ ] **Dashboard** com mÃ©tricas histÃ³ricas

## ğŸ¤ Contribuindo

1. **Fork** o projeto
2. **Crie** uma branch: `git checkout -b feature/nova-funcionalidade`
3. **Commit** suas mudanÃ§as: `git commit -m 'Add nova funcionalidade'`
4. **Push** para a branch: `git push origin feature/nova-funcionalidade`
5. **Abra** um Pull Request

## ğŸ“„ LicenÃ§a

Este projeto estÃ¡ sob a licenÃ§a MIT. Veja o arquivo LICENSE para mais detalhes.

---

**ğŸš€ Desenvolvido para automatizar e monitorar o scraping de questÃµes de concurso com interface web moderna e tempo real!** 