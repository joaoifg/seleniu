#!/usr/bin/env python3
"""
Script para iniciar o monitor web e opcionalmente o scraper.
"""

import os
import sys
import time
import argparse
import subprocess
import threading
from pathlib import Path

def run_web_interface():
    """Executa a interface web."""
    print("ğŸŒ Iniciando interface web...")
    subprocess.run([sys.executable, "web_interface.py"])

def run_scraper():
    """Executa o scraper."""
    print("ğŸ” Iniciando scraper...")
    time.sleep(3)  # Aguarda interface web subir
    subprocess.run([sys.executable, "scraper.py"])

def check_credentials():
    """Verifica se as credenciais estÃ£o configuradas."""
    from dotenv import load_dotenv
    
    load_dotenv()
    email = os.getenv("QC_EMAIL")
    password = os.getenv("QC_PASSWORD")
    
    if not email or not password:
        print("âŒ Credenciais nÃ£o configuradas!")
        print("Execute: python setup_env.py")
        return False
    
    return True

def check_urls_file():
    """Verifica se o arquivo urls.txt existe."""
    if not Path("urls.txt").exists():
        print("âŒ Arquivo urls.txt nÃ£o encontrado!")
        print("Crie o arquivo urls.txt com as URLs a serem processadas.")
        return False
    
    urls = [u.strip() for u in Path("urls.txt").read_text().splitlines() if u.strip()]
    if not urls:
        print("âŒ Arquivo urls.txt estÃ¡ vazio!")
        return False
    
    print(f"âœ… Encontradas {len(urls)} URLs para processar")
    return True

def main():
    parser = argparse.ArgumentParser(description="Monitor do QConcursos Scraper")
    parser.add_argument("--mode", choices=["web", "scraper", "both"], default="both",
                      help="Modo de execuÃ§Ã£o: apenas web, apenas scraper, ou ambos")
    parser.add_argument("--no-check", action="store_true",
                      help="Pula verificaÃ§Ãµes de prÃ©-requisitos")
    
    args = parser.parse_args()
    
    print("ğŸš€ QConcursos Scraper Monitor")
    print("=" * 40)
    
    # VerificaÃ§Ãµes de prÃ©-requisitos
    if not args.no_check:
        print("ğŸ” Verificando prÃ©-requisitos...")
        
        if args.mode in ["scraper", "both"]:
            if not check_credentials():
                return 1
            
            if not check_urls_file():
                return 1
        
        print("âœ… VerificaÃ§Ãµes concluÃ­das!")
        print()
    
    # Cria diretÃ³rios necessÃ¡rios
    for dir_name in ["logs", "screenshots", "output"]:
        Path(dir_name).mkdir(exist_ok=True)
    
    if args.mode == "web":
        print("ğŸŒ Executando apenas a interface web...")
        print("ğŸ“Š Acesse: http://localhost:5000")
        run_web_interface()
        
    elif args.mode == "scraper":
        print("ğŸ” Executando apenas o scraper...")
        run_scraper()
        
    elif args.mode == "both":
        print("ğŸš€ Executando interface web e scraper...")
        print("ğŸ“Š Acesse: http://localhost:5000")
        print()
        
        # Inicia interface web em thread separada
        web_thread = threading.Thread(target=run_web_interface, daemon=True)
        web_thread.start()
        
        # Aguarda um pouco e depois inicia o scraper
        time.sleep(3)
        
        try:
            run_scraper()
        except KeyboardInterrupt:
            print("\nâ›” Interrompido pelo usuÃ¡rio")
            return 0
    
    return 0

if __name__ == "__main__":
    sys.exit(main()) 